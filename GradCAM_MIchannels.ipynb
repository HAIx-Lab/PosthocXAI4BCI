{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "gpus = [1]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import scipy.io\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import torch.autograd as autograd\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# from common_spatial_pattern import csp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from matplotlib import mlab as mlab\n",
    "from torch.backends import cudnn\n",
    "from utils import GradCAM, show_cam_on_image\n",
    "\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "import myimporter\n",
    "from BCI_functions import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the overall model class, omitted here\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, depth=2, n_classes=2, **kwargs):\n",
    "        super().__init__(\n",
    "            # ... the model\n",
    "        )\n",
    "        \n",
    "# ! A crucial step for adaptation on Transformer\n",
    "# reshape_transform  b 61 40 -> b 40 1 61\n",
    "def reshape_transform(tensor):\n",
    "    result = rearrange(tensor, 'b (h w) e -> b e (h) (w)', h=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84acb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution module\n",
    "# use conv to capture local features, instead of postion embedding.\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (7, 1), (1, 1)), # 22 when using 64 channels\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.shallownet(x.float())\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=10,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # global average pooling\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2200, 256), # 25800 for 2s, 8600 for 1s # for 64 #2200 for 17 # 3000 for 21 \n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, n_classes) #4 # change here for classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conformer(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, depth=2, n_classes=2, **kwargs):\n",
    "        super().__init__(\n",
    "\n",
    "            PatchEmbedding(emb_size),\n",
    "            TransformerEncoder(depth, emb_size),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This class if has list of subject id can later support combination of sub ids\n",
    "# TODO: add a function transform to convert dataset to train test, avoiding repetition of same code\n",
    "\n",
    "class EEGMMIDTrSet(Data.Dataset):\n",
    "    def __init__(self, subject_id, transform=None):\n",
    "        root_dir = \"../../Deep-Learning-for-BCI/dataset/\"\n",
    "        dataset_raw = np.load(root_dir + str(subject_id) + '.npy')\n",
    "        dataset=[]  # feature after filtering\n",
    "\n",
    "        # EEG Gamma pattern decomposition\n",
    "        for i in range(dataset_raw[:,:-1].shape[1]):\n",
    "            x = dataset_raw[:, i]\n",
    "            fs = 160.0\n",
    "            lowcut = 8.0\n",
    "            highcut = 30.0\n",
    "            y = butter_bandpass_filter(x, lowcut, highcut, fs, order=3)\n",
    "            dataset.append(y)\n",
    "        dataset=np.array(dataset).T\n",
    "        dataset=np.hstack((dataset,dataset_raw[:,-1:]))\n",
    "        print(dataset.shape)\n",
    "        # keep 4,5 which are left and right fist open close imagery classes, remove rest\n",
    "        # refer 1-Data.ipynb for the details\n",
    "        removed_label = [0,1,6,7,8,9,10]  # [0,1,2,3,4,5,10] for hf # [0,1,6,7,8,9,10] for lr\n",
    "        for ll in removed_label:\n",
    "            id = dataset[:, -1]!=ll\n",
    "            dataset = dataset[id]\n",
    "\n",
    "        # Pytorch needs labels to be sequentially ordered starting from 0\n",
    "        dataset[:, -1][dataset[:, -1] == 2] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 4] = 0\n",
    "        dataset[:, -1][dataset[:, -1] == 3] = 1\n",
    "        dataset[:, -1][dataset[:, -1] == 5] = 1\n",
    "        \n",
    "        # data segmentation\n",
    "        n_class = 2 #int(11-len(removed_label))  # 0~9 classes ('10:rest' is not considered)\n",
    "        no_feature = 64  # the number of the features\n",
    "        segment_length = 160 #160  # selected time window; 16=160*0.1\n",
    "        \n",
    "        #Overlapping is removed to avoid training set overlap with test set\n",
    "        data_seg = extract(dataset, n_classes=n_class, n_fea=no_feature, \n",
    "                           time_window=segment_length, moving=(segment_length))  # /2 for 50% overlapping\n",
    "        print('After segmentation, the shape of the data:', data_seg.shape)\n",
    "\n",
    "        # split training and test data\n",
    "        no_longfeature = no_feature*segment_length\n",
    "        data_seg_feature = data_seg[:, :no_longfeature]\n",
    "        self.data_seg_label = data_seg[:, no_longfeature:no_longfeature+1]\n",
    "        \n",
    "        # Its important to have random state set equal for Training and test dataset\n",
    "        train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "            data_seg_feature, self.data_seg_label,random_state=0, shuffle=True,stratify=self.data_seg_label)\n",
    "\n",
    "        # Check the class label splits to maintain balance\n",
    "        unique, counts = np.unique(self.data_seg_label, return_counts=True)\n",
    "        left_perc = counts[0]/sum(counts)\n",
    "        if left_perc < 0.4 or left_perc > 0.6:\n",
    "            print(\"Imbalanced dataset with split of: \",left_perc,1-left_perc)\n",
    "        else:\n",
    "            print(\"Classes balanced.\")\n",
    "        unique, counts = np.unique(train_label, return_counts=True)\n",
    "        print(\"Class label splits in training set \\n \",np.asarray((unique, counts)).T)\n",
    "        unique, counts = np.unique(test_label, return_counts=True)\n",
    "        print(\"Class label splits in test set\\n \",np.asarray((unique, counts)).T)\n",
    "\n",
    "\n",
    "\n",
    "        # normalization\n",
    "        # before normalize reshape data back to raw data shape\n",
    "        train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "        test_feature_2d = test_feature.reshape([-1, no_feature])\n",
    "\n",
    "        scaler1 = StandardScaler().fit(train_feature_2d)\n",
    "        train_fea_norm1 = scaler1.transform(train_feature_2d) # normalize the training data\n",
    "        test_fea_norm1 = scaler1.transform(test_feature_2d) # normalize the test data\n",
    "        print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "#         # This is to select only MI related channels\n",
    "#         train_fea_norm1 = train_fea_norm1[:,:21]\n",
    "#         test_fea_norm1 = test_fea_norm1[:,:21]\n",
    "#         no_feature = 21\n",
    "#         [20, 21, 22, 23, 27, 28, 29, 37, 43, 53, 54, 56, 58, 59, 61, 62, 63] # top fr for 109 _top16\n",
    "        train_fea_norm1 = train_fea_norm1[:,[20, 21, 22, 23, 27, 28, 29, 37, 43, 53, 54, 56, 58, 59, 61, 62, 63]]\n",
    "        test_fea_norm1 = test_fea_norm1[:,[20, 21, 22, 23, 27, 28, 29, 37, 43, 53, 54, 56, 58, 59, 61, 62, 63]]\n",
    "        no_feature = 17\n",
    "        \n",
    "        # after normalization, reshape data to 3d\n",
    "        train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "        print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "              '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n",
    "        \n",
    "        # reshape for data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        # earlier it was (trial,timesamples,electrode_channel)\n",
    "        train_fea_reshape1 = np.swapaxes(np.expand_dims(train_fea_norm1,1),2,3)\n",
    "        test_fea_reshape1 = np.swapaxes(np.expand_dims(test_fea_norm1,1),2,3)\n",
    "        print('After expand dims, the shape of training feature:', train_fea_reshape1.shape,\n",
    "              '\\nAfter expand dims, the shape of test feature:', test_fea_reshape1.shape)\n",
    "        \n",
    "        self.data = torch.tensor(train_fea_reshape1)\n",
    "        self.targets = torch.tensor(train_label.flatten()).long()\n",
    "        \n",
    "        print(\"data and target type:\",type(self.data),type(self.targets))\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = self.data[idx], self.targets[idx]\n",
    "        return data, target\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_weights=class_weight.compute_class_weight('balanced',np.unique(self.data_seg_label),\n",
    "                                                        self.data_seg_label[:,0])\n",
    "        return class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f7efc",
   "metadata": {},
   "source": [
    "## Get topo plots for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "device = torch.device(\"cpu\") #cpu\n",
    "model = Conformer(n_classes=2)\n",
    "cat_dict = {0:'left_hand',1:'right_hand'}\n",
    "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n",
    "index = [20, 21, 22, 23, 27, 28, 29, 37, 43, 53, 54, 56, 58, 59, 61, 62, 63] # top17 FR for 16_109 subs \n",
    "biosemi_montage.ch_names = [biosemi_montage.ch_names[i] for i in index]\n",
    "biosemi_montage.dig = [biosemi_montage.dig[i+3] for i in index]\n",
    "info = mne.create_info(ch_names=biosemi_montage.ch_names, sfreq=160,ch_types='eeg')\n",
    "\n",
    "\n",
    "for sub_id in [42]:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 25)) #figsize=(15, 5)\n",
    "    model.load_state_dict(torch.load(\"../transformer_results/eegmmid_ws_offline_\"+ str(sub_id)+ \"_model1\" + '.pth', map_location=device)) \n",
    "    target_layers = [model[1]]  # set the target layer \n",
    "    print(target_layers)\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False, reshape_transform=reshape_transform)\n",
    "\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "\n",
    "\n",
    "    data = train_ds.data#[:, :, :22, :]allData#\n",
    "    \n",
    "    \n",
    "    for target_category in range(2):\n",
    "        \n",
    "        all_cam = []\n",
    "        # this loop is used to obtain the cam of each trial/sample\n",
    "        for i in range(data.shape[0]):\n",
    "            test = torch.as_tensor(data[i:i+1, :, :, :], dtype=torch.float32)\n",
    "            test = torch.autograd.Variable(test, requires_grad=True)\n",
    "\n",
    "            grayscale_cam = cam(input_tensor=test,target_category=target_category) #,target_category=2\n",
    "            print(grayscale_cam.shape)\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "            all_cam.append(grayscale_cam)\n",
    "\n",
    "        # the mean of all data\n",
    "        test_all_data = np.squeeze(np.mean(data.detach().cpu().numpy(), axis=0)) #.detach().cpu().numpy()\n",
    "        test_all_data = (test_all_data - np.mean(test_all_data)) / np.std(test_all_data)\n",
    "        mean_all_test = np.mean(test_all_data, axis=1)\n",
    "\n",
    "        # the mean of all cam\n",
    "        test_all_cam = np.mean(all_cam, axis=0)\n",
    "        mean_all_cam = np.mean(test_all_cam, axis=1)\n",
    "\n",
    "        # apply cam on the input data\n",
    "        hyb_all = test_all_data * test_all_cam\n",
    "        hyb_all = (hyb_all - np.mean(hyb_all)) / np.std(hyb_all)\n",
    "        mean_hyb_all = np.mean(hyb_all, axis=1)\n",
    "\n",
    "        evoked = mne.EvokedArray(test_all_data, info)\n",
    "        evoked.set_montage(biosemi_montage)\n",
    "        if target_category == 0:\n",
    "            # Create a topomap for the current oscillation band\n",
    "            im,cn = mne.viz.plot_topomap(mean_all_test, evoked.info,axes=axes[target_category], \n",
    "                                 show=False, res=600, size=5);\n",
    "\n",
    "            # Set the plot title\n",
    "            axes[target_category].set_title(\"Raw\", {'fontsize' : 10})\n",
    "            divider = make_axes_locatable(axes[target_category])\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "\n",
    "        # Create a topomap for the current oscillation band\n",
    "        im,cn = mne.viz.plot_topomap(mean_hyb_all, evoked.info,axes=axes[target_category+1], \n",
    "                             show=False, res=600, size=5);\n",
    "\n",
    "        # Set the plot title\n",
    "        axes[target_category+1].set_title(\"S\" + str(sub_id) + \"_\" + cat_dict[target_category], {'fontsize' : 10})\n",
    "\n",
    "        divider = make_axes_locatable(axes[target_category+1])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    \n",
    "    plt.savefig('./outimgs/'+str(sub_id) + '_topo.png')\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51702a",
   "metadata": {},
   "source": [
    "## Find most relevant channels for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "device = torch.device(\"cpu\")\n",
    "model = Conformer(n_classes=2)\n",
    "cat_dict = {0:'left hand movement',1:'right hand movement'}\n",
    "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n",
    "index = [8, 9, 10, 46, 45, 44, 43, 13, 12, 11, 47, 48, 49, 50, 16, 17, 18, \n",
    "         31, 55, 54, 53]\n",
    "biosemi_montage.ch_names = [biosemi_montage.ch_names[i] for i in index]\n",
    "biosemi_montage.dig = [biosemi_montage.dig[i+3] for i in index]\n",
    "info = mne.create_info(ch_names=biosemi_montage.ch_names, sfreq=160,ch_types='eeg')\n",
    "\n",
    "top_channel_dict = {}\n",
    "\n",
    "for sub_id in [7,15,29,32,35,42,43,46,48,49,54,56,62,93,94,108]:\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"../transformer_results/eegmmid_ws_offline_\"+ str(sub_id)+ \"_model0\" + '.pth', map_location=device)) \n",
    "    target_layers = [model[1]]  # set the target layer \n",
    "    print(target_layers)\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False, reshape_transform=reshape_transform)\n",
    "\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "\n",
    "\n",
    "    data = train_ds.data\n",
    "    \n",
    "    \n",
    "    for target_category in range(2):\n",
    "        \n",
    "        all_cam = []\n",
    "        # this loop is used to obtain the cam of each trial/sample\n",
    "        for i in range(data.shape[0]):\n",
    "            test = torch.as_tensor(data[i:i+1, :, :, :], dtype=torch.float32)\n",
    "            test = torch.autograd.Variable(test, requires_grad=True)\n",
    "\n",
    "            grayscale_cam = cam(input_tensor=test,target_category=target_category) #,target_category=2\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "            all_cam.append(grayscale_cam)\n",
    "\n",
    "        # the mean of all data\n",
    "        test_all_data = np.squeeze(np.mean(data.detach().cpu().numpy(), axis=0)) #.detach().cpu().numpy()\n",
    "        test_all_data = (test_all_data - np.mean(test_all_data)) / np.std(test_all_data)\n",
    "        mean_all_test = np.mean(test_all_data, axis=1)\n",
    "\n",
    "        # the mean of all cam\n",
    "        test_all_cam = np.mean(all_cam, axis=0)\n",
    "        mean_all_cam = np.mean(test_all_cam, axis=1)\n",
    "\n",
    "        # apply cam on the input data\n",
    "        hyb_all = test_all_data * test_all_cam\n",
    "        hyb_all = (hyb_all - np.mean(hyb_all)) / np.std(hyb_all)\n",
    "        mean_hyb_all = np.mean(hyb_all, axis=1)\n",
    "        \n",
    "        # get top 3 channel index -> feature relevance\n",
    "        nind = np.argsort(mean_hyb_all)[-10:]\n",
    "        print([biosemi_montage.ch_names[i] for i in nind])\n",
    "        \n",
    "        imp_ind = [mean_hyb_all[i] for i in nind]\n",
    "\n",
    "        ch_names = [biosemi_montage.ch_names[i] for i in nind]\n",
    "        print(imp_ind)\n",
    "        if not math.isnan(imp_ind[0]):\n",
    "            top_channel_dict[(sub_id,target_category)] = ch_names \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_list = []\n",
    "right_list = []\n",
    "for key in top_channel_dict:\n",
    "    if key[1] == 0:\n",
    "        left_list.extend(top_channel_dict[key])\n",
    "    elif key[1] == 1:\n",
    "        right_list.extend(top_channel_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cae91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter,OrderedDict\n",
    "freq_left = Counter(left_list)\n",
    "freq_right = Counter(right_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f75160",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "plt.xticks(rotation = 45)\n",
    "ax.tick_params(axis='x', which='major')\n",
    "ax.set_xlabel('Channel Names', fontsize=10)\n",
    "ax.set_ylabel('Frequency of appearances in Top 10 features from GradCAM', fontsize=10)\n",
    "plt.bar(OrderedDict(freq_left.most_common()).keys(), OrderedDict(freq_left.most_common()).values())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "# fig = plt.figure(figsize=(15,6))\n",
    "plt.xticks(rotation = 45)\n",
    "ax.tick_params(axis='x', which='major')\n",
    "ax.set_xlabel('Channel Names', fontsize=10)\n",
    "ax.set_ylabel('Frequency of appearances in Top 10 features from GradCAM', fontsize=10)\n",
    "plt.bar(OrderedDict(freq_right.most_common()).keys(), OrderedDict(freq_right.most_common()).values())\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"left: \",freq_left.most_common()[:10])\n",
    "print(\"right: \",freq_right.most_common()[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8414e",
   "metadata": {},
   "source": [
    "## Code to generate TF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37056e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nz, F9, F10, FT9, FT10, A1, A2, TP9, TP10, P9, and P10\n",
    "# [22,25,26,33,32,31,30,39,1,2,3,10,9,8,41,45,15,16,17,50,49,48,47]\n",
    "print(pre_montage.ch_names)\n",
    "\n",
    "final_index = []\n",
    "data_seq_ch = ['FC5','FC3','FC1','FCz','FC2','FC4','FC6','C5',\n",
    "              'C3','C1','Cz','C2','C4','C6','CP5','CP3','CP1',\n",
    "              'CPz','CP2','CP4','CP6','Fp1','Fpz','Fp2','AF7',\n",
    "              'AF3','AFz','AF4','AF8','F7','F5','F3','F1','Fz',\n",
    "              'F2','F4','F6','F8','FT7','FT8','T7','T8','P9','P10','TP7','TP8','P7','P5','P3','P1','Pz','P2',\n",
    "              'P4','P6','P8','PO7','PO3','POz','PO4','PO8','O1',\n",
    "              'Oz','O2','Iz'] #'T9','T10' removed and using P9,P10 instead\n",
    "\n",
    "for ch_n in data_seq_ch:\n",
    "    final_index.append(pre_montage.ch_names.index(ch_n))\n",
    "print(final_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from mne.time_frequency import tfr_morlet\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = Conformer(n_classes=2)\n",
    "cat_dict = {0:'left hand movement',1:'right hand movement'}\n",
    "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n",
    "index = [8, 9, 10, 11, 12, 13, 16, 17, 18, 31, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55]\n",
    "#[20, 21, 22, 23, 27, 28, 29, 37, 43, 53, 54, 56, 58, 59, 61, 62, 63] # For 109 subs all channels topfr \n",
    "# [18, 20, 21, 22, 23, 27, 28, 29, 34, 38, 44, 53, 54, 55, 59, 60, 61, 63] # old\n",
    "# [8, 9, 10, 11, 12, 13, 16, 17, 18, 31, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55] 21 MI channels\n",
    "#range(64)#[37, 9, 10, 46, 45, 44, 13, 12, 11, 47, 48, 49, 50, 17, 18, 31, 55, 54, 19, 30, 56, 29]  # for bci competition iv 2a\n",
    "\n",
    "biosemi_montage.ch_names = [biosemi_montage.ch_names[i] for i in index]\n",
    "biosemi_montage.dig = [biosemi_montage.dig[i+3] for i in index]\n",
    "info = mne.create_info(ch_names=biosemi_montage.ch_names, sfreq=160,ch_types='eeg')\n",
    "\n",
    "top_channel_dict = {}\n",
    "# fig, axes = plt.subplots(1, 2) #figsize=(15, 5)\n",
    "# print(axes)\n",
    "for sub_id in [42]:#range(7,8):\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"../transformer_results/eegmmid_ws_offline_\"+ str(sub_id)+ \"_model0\" + '.pth', map_location=device)) \n",
    "    #./eegmmid_ws_offline_1_model0.pth ../model.pth\n",
    "    target_layers = [model[1]]  # set the target layer \n",
    "    print(target_layers)\n",
    "    cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False, reshape_transform=reshape_transform)\n",
    "\n",
    "    train_ds = EEGMMIDTrSet(subject_id=sub_id)\n",
    "\n",
    "\n",
    "    data = train_ds.data#[:, :, :22, :]allData#\n",
    "#     print(data.shape)\n",
    "    \n",
    "    \n",
    "    for target_category in range(2):\n",
    "        # # used for cnn model without transformer\n",
    "# model.load_state_dict(torch.load('./model/model_cnn.pth', map_location=device))\n",
    "# target_layers = [model[0].projection]  # set the layer you want to visualize, you can use torchsummary here to find the layer index\n",
    "# cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "        \n",
    "        all_cam = []\n",
    "        # this loop is used to obtain the cam of each trial/sample\n",
    "        for i in range(data.shape[0]):\n",
    "            test = torch.as_tensor(data[i:i+1, :, :, :], dtype=torch.float32)\n",
    "            test = torch.autograd.Variable(test, requires_grad=True)\n",
    "\n",
    "            grayscale_cam = cam(input_tensor=test,target_category=target_category)\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "            all_cam.append(grayscale_cam)\n",
    "\n",
    "        # the mean of all data\n",
    "        test_all_data = np.squeeze(np.mean(data.detach().cpu().numpy(), axis=0)) #.detach().cpu().numpy()\n",
    "        test_all_data = (test_all_data - np.mean(test_all_data)) / np.std(test_all_data)\n",
    "        mean_all_test = np.mean(test_all_data, axis=1)\n",
    "\n",
    "        # the mean of all cam\n",
    "        test_all_cam = np.mean(all_cam, axis=0)\n",
    "#         test_all_cam = (test_all_cam - np.mean(test_all_cam)) / np.std(test_all_cam)\n",
    "        mean_all_cam = np.mean(test_all_cam, axis=1)\n",
    "\n",
    "        # apply cam on the input data\n",
    "        hyb_all = test_all_data * test_all_cam\n",
    "        hyb_all = (hyb_all - np.mean(hyb_all)) / np.std(hyb_all)\n",
    "        mean_hyb_all = np.mean(hyb_all, axis=0)\n",
    "        print(mean_hyb_all.shape)\n",
    "        print(mean_hyb_all)\n",
    "        \n",
    "        nind = np.argsort(mean_hyb_all)[-3:]\n",
    "        print(\"final_timespan\",nind/160)\n",
    "        time_freq = list(itertools.product(nind/160,[10,21]))\n",
    "        freqs = np.logspace(*np.log10([8, 30]), num=8)\n",
    "        n_cycles = freqs / 2.0  # different number of cycle per frequency\n",
    "#         print(test_all_data.shape)\n",
    "#         single_info = mne.create_info(ch_names=[biosemi_montage.ch_names], sfreq=160,ch_types='eeg')\n",
    "        evoked = mne.EvokedArray(test_all_data, info)\n",
    "        evoked.set_montage(biosemi_montage)\n",
    "        power = tfr_morlet(\n",
    "            evoked,\n",
    "            freqs=freqs,\n",
    "            n_cycles=n_cycles,\n",
    "            use_fft=True,\n",
    "            return_itc=False,\n",
    "            decim=3,\n",
    "            n_jobs=None,\n",
    "        )\n",
    "        power.plot_joint(baseline=(0, 0), mode=\"mean\", tmin=0, tmax=1, \n",
    "                         timefreqs=time_freq)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
